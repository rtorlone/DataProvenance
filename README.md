# DataProvenance
This repository contains the project code related to capturing data provenance in the context of data preprocessing in data science, as described in the paper "*Capturing and querying fine-grained provenance of preprocessing pipelines in data science*", to appear in PVLDB. The code is written in [Python](https://www.python.org/).

## Organization of the repository
This repository is organized in two main sections (folders):
1. [prov_acquisition](prov_acquisition/) contains the part relating to the acquisition of data provenance. This folder is divided into three other sections: 
 Questa cartella Ã¨ suddivisa in altre tre sezioni:
    * [prov_libraries](prov_acquisition/prov_libraries/) - the two implementations for acquiring data provenance.
    * [real_world_pipeline](prov_acquisition/real_world_pipeline/) - acquisition of the provenance of data on three real pipelines involving different types of preprocessing steps.
    * [preprocessing_methods](prov_acquisition/preprocessing_methods/) - 
    examples of provenance acquisition for each preprocessing operation implemented on datasets generated by *DIGen*, the data generator provided by the [TPC] benchmark(http://www.tpc.org/tpcdi/).
2. [queries](queries/) contains the code for querying the data provenance.

## Libraries needed to run the code

* [PROV Python library](https://pypi.org/project/prov/): 
an implementation of the World Wide Web [Provenance Data Model](https://www.w3.org/TR/prov-dm/).
* [pymongo](https://pymongo.readthedocs.io/en/stable/)
* [pandas](https://pandas.pydata.org/)

## Execution

### Acquisition
1. To run the examples on the three real pipelines: 
    * Go to the  *prov_acquisition/*  folder
    * Run the file with the python command `python real_world_pipeline/*.py` 
    followed by `-op` if you want to use the optimized implementation. For example, **python real_world_pipeline/GermanCleanup_prov.py -op**
    
    
The result will be stored in the *prov_acquisition/prov_results/<nome_dataset>* folder.
For example, **prov_acquisition/prov_results/German**

2. For examples using DIGen generated datasets: 
    * Go to folder  *prov_acquisition/* 
    * Run the file with the python command `python preprocessing_methods/*.py -i <input_file>` followed by `-op` if you want to use the optimized implementation. For example, **python preprocessing_methods/FT_prov.py -i datasets/Trade_SF3.csv -op**
    
Again the result will be stored in the *prov_acquisition/prov_results/<nome_dataset>* folder.
For example, **prov_acquisition/prov_results/Trade_SF3**.

NOTE: TPC-DI datasets are not present in this repository. They must be generated through the DIGen generator.


### Query
To be able to query the data provenance, the data created in the acquisition phase is inserted into the [MongoDB](https://www.mongodb.com/) database:
1. Create the database with `python queries/create_mongodb.py <db_name> <files_path>` command.
For example, **python queries/create_mongodb.py German prov_acquisition/prov_results/German**
2. In order to execute the queries it is also necessary to generate the output collection that contains the output entities of the preprocessing pipeline. They also created the indexes to optimize the queries.
      * Go to folder  *queries/*
      * Run the python command `python get_output_entities.py <db_name>`
      For example, **python get_output_entities.py German**
3. Finally, you can run queries with the python command `python *.py <db_name>`
      For example, **python all_transformations.py German**
      
